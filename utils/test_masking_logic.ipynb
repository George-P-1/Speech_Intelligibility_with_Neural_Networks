{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Masking Logic to Ignore Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55, 0.72, 0.6 , 0.54, 0.42],\n",
       "       [0.65, 0.44, 0.89, 0.96, 0.38],\n",
       "       [0.79, 0.53, 0.57, 0.93, 0.07]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# print the array with 2 decimal places\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Generate a random 2D array\n",
    "np.random.seed(0)\n",
    "a = np.random.rand(3, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55, 0.72, 0.6 , 0.54, 0.42, 0.  , 0.  ],\n",
       "       [0.65, 0.44, 0.89, 0.96, 0.38, 0.  , 0.  ],\n",
       "       [0.79, 0.53, 0.57, 0.93, 0.07, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 2 columns to the array and fill them with zeros\n",
    "b = np.zeros((3, 2))\n",
    "c = np.concatenate((a, b), axis=1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "[[1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0.]] \n",
      "shape: (3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Generate a mask with 1s and 0s for the non padded columns of the array\n",
    "mask = np.ones((3, 5+2), dtype=float)\n",
    "mask[:, -2:] = 0\n",
    "print(f\"Mask:\\n{mask} \\nshape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of non-padded columns using the only the mask matrix\n",
    "non_padded_columns = np.sum(mask == 1) / mask.shape[0]\n",
    "non_padded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masks:\n",
      "[[[1. 1. 1. 1. 1. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]]] \n",
      "shape: (4, 3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create a new masks with all ones for now\n",
    "masks = np.ones((3, 5+2), dtype=float)\n",
    "# Make a variable called masks that has an additional dimension in the beginning of size 2\n",
    "# So 4 x 7 x 3, where 4 is batch size and 7 is columns and 3 is rows\n",
    "masks = np.expand_dims(masks, 0)\n",
    "masks = np.repeat(masks, 4, axis=0)\n",
    "# edit the masks of different batch elements\n",
    "masks[0][:, -2:] = 0\n",
    "masks[1][:, -1] = 0\n",
    "masks[2][:, -3:] = 0\n",
    "\n",
    "print(f\"Masks:\\n{masks} \\nshape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 6., 4., 7.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of non-padded columns for each batch element using the variable of dimension: batch size x rows x columns\n",
    "non_padded_columns = np.sum(masks == 1, axis=(1, 2)) / masks.shape[1]\n",
    "non_padded_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of GRU network, the `non-padded_columns` variable can be used a `variable_sequence_length` variable that is passed to the forward function to ignore the padded values. This way loss won't be calculated for the padded values as the padded values aren't forwarded to the model in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also when comparing this example to dmatrix masks, the `masks.shape[1]` contains the octave bands dimension so that is what you have to divide the sum of all 1s in the mask matrix with. This gives the number of time frames that are not padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss array:\n",
      "[[0.09 0.02 0.83 0.78 0.87 0.98 0.8 ]\n",
      " [0.46 0.78 0.12 0.64 0.14 0.94 0.52]\n",
      " [0.41 0.26 0.77 0.46 0.57 0.02 0.62]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the loss array with random number\n",
    "loss = np.random.rand(3, 5+2)\n",
    "print(f\"Loss array:\\n{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss array after multiplication:\n",
      "[[[0.09 0.02 0.83 0.78 0.87 0.   0.  ]\n",
      "  [0.46 0.78 0.12 0.64 0.14 0.   0.  ]\n",
      "  [0.41 0.26 0.77 0.46 0.57 0.   0.  ]]\n",
      "\n",
      " [[0.09 0.02 0.83 0.78 0.87 0.98 0.  ]\n",
      "  [0.46 0.78 0.12 0.64 0.14 0.94 0.  ]\n",
      "  [0.41 0.26 0.77 0.46 0.57 0.02 0.  ]]\n",
      "\n",
      " [[0.09 0.02 0.83 0.78 0.   0.   0.  ]\n",
      "  [0.46 0.78 0.12 0.64 0.   0.   0.  ]\n",
      "  [0.41 0.26 0.77 0.46 0.   0.   0.  ]]\n",
      "\n",
      " [[0.09 0.02 0.83 0.78 0.87 0.98 0.8 ]\n",
      "  [0.46 0.78 0.12 0.64 0.14 0.94 0.52]\n",
      "  [0.41 0.26 0.77 0.46 0.57 0.02 0.62]]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply loss by masks\n",
    "loss = loss * masks\n",
    "print(f\"Loss array after multiplication:\\n{loss}\")\n",
    "\n",
    "# loss = (loss * masks.mean(dim=2)).sum() / masks.sum() # This is how it is done in train.py for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_padded_columns:\n",
      "tensor([5, 6, 4, 7]) \n",
      "shape: torch.Size([4])\n",
      "Input array shape:\n",
      "(4, 3, 7)\n",
      "Input array shape after removing padding:\n",
      "(4, 3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Review variables\n",
    "print(f\"non_padded_columns:\\n{non_padded_columns} \\nshape: {non_padded_columns.shape}\")\n",
    "\n",
    "# Create a variable that contains input in the dimension of batch size x rows x columns in this case 4 x 3 x 7\n",
    "input = np.random.rand(4, 3, 5+2)\n",
    "# print(f\"Input array:\\n{input}\")\n",
    "print(f\"Input array shape:\\n{input.shape}\")\n",
    "\n",
    "# Use the non_padded_columns variable to remove the padding from the input variable for each batch element\n",
    "# Remember that now the columns dimension will have different size for each batch element (maybe use torch tensor for this)\n",
    "#TODO -  Can't figure out how to do this\n",
    "\n",
    "# print(f\"Input array after removing padding:\\n{input}\")\n",
    "print(f\"Input array shape after removing padding:\\n{input.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something about packing and unpacking sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([4, 3, 7])\n",
      "Mask tensor shape: torch.Size([4, 3, 7])\n",
      "Non-padded time steps per batch: tensor([5, 6, 4, 7])\n",
      "Input tensor shape after permutation: torch.Size([4, 7, 3])\n",
      "Packed input: tensor([[0.8792, 0.5305, 0.2791],\n",
      "        [0.9680, 0.1313, 0.5922],\n",
      "        [0.6898, 0.7488, 0.5024],\n",
      "        [0.2484, 0.9242, 0.7401],\n",
      "        [0.5437, 0.9221, 0.2097],\n",
      "        [0.9944, 0.6041, 0.8968],\n",
      "        [0.0584, 0.2378, 0.9426],\n",
      "        [0.5059, 0.8623, 0.6805],\n",
      "        [0.2827, 0.0895, 0.1157],\n",
      "        [0.4518, 0.3828, 0.4067],\n",
      "        [0.7307, 0.1719, 0.6340],\n",
      "        [0.3104, 0.0487, 0.6224],\n",
      "        [0.0302, 0.4059, 0.5771],\n",
      "        [0.0709, 0.8954, 0.5521],\n",
      "        [0.8817, 0.4493, 0.8673],\n",
      "        [0.3730, 0.2536, 0.7105],\n",
      "        [0.7103, 0.0243, 0.6953],\n",
      "        [0.2928, 0.9678, 0.2717],\n",
      "        [0.2724, 0.3045, 0.9402],\n",
      "        [0.0079, 0.3426, 0.6720],\n",
      "        [0.1524, 0.5469, 0.4554],\n",
      "        [0.3727, 0.6222, 0.9489]])\n",
      "Packed input data: torch.Size([22, 3])\n",
      "Packed input batch_sizes: tensor([4, 4, 4, 4, 3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert input and mask to PyTorch tensors\n",
    "input_tensor = torch.tensor(input, dtype=torch.float32)  # Shape: (batch_size, rows, cols)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "mask_tensor = torch.tensor(masks, dtype=torch.float32)  # Shape: (batch_size, rows, cols)\n",
    "print(f\"Mask tensor shape: {mask_tensor.shape}\")\n",
    "\n",
    "# Compute the number of non-padded time steps per batch element (sum along time axis)\n",
    "non_padded_lengths = torch.sum(mask_tensor[:, 0, :] == 1, dim=1)  # Shape: (batch_size,)\n",
    "\n",
    "print(f\"Non-padded time steps per batch: {non_padded_lengths}\")\n",
    "\n",
    "# Permute input to (batch, sequence_length, feature_dim) for GRU compatibility\n",
    "input_tensor = input_tensor.permute(0, 2, 1)  # Shape: (batch_size, sequence_length, feature_dim)\n",
    "print(f\"Input tensor shape after permutation: {input_tensor.shape}\")\n",
    "\n",
    "# Pack sequences for GRU\n",
    "packed_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "    input_tensor, non_padded_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    ")\n",
    "\n",
    "print(f\"Packed input: {packed_input.data}\")\n",
    "print(f\"Packed input data: {packed_input.data.shape}\")\n",
    "print(f\"Packed input batch_sizes: {packed_input.batch_sizes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
